{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Gradient Descent\n",
    "\n",
    "Linear regression allows us to make numerical predictions based on one or more inputs, either numerical or categorical. Logistic regression is the equivalent that allows us to make classification predictions - predicting if something falls into group A or group B. Logistic regression (and other classification methods) are extremely common. Regression/prediction and classification are the two big pillars of predictive analytics that we will look at through next term. \n",
    "\n",
    "One thing that we can think of to get started is how to visualize a one feature logistic regression. In a linear regression the line, or model, was a prediction of value - with an input of X, we predict a value of Y. When plotting a logistic regression, the line, or model, is a dividing line between two classes. With an input of X, we predict a class of A or B.\n",
    "\n",
    "![Logistic Regression](images/log_reg.png \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classification Problem\n",
    "\n",
    "Dealing with classification is a little different than regression, because now we are not looking to predict a value, we are looking to predic a class - or phrased alternatively, we are looking to divide two (or more) sets of data.  \n",
    "\n",
    "If we plot a simple 2 varaible problem, just like we did in linear regression, we'll get something that looks like this:\n",
    "<ul>\n",
    "<li> Suppose that BMI is our X and Outcome (do you have diabetes?) is the Y. \n",
    "<li> Plot that on a scatter plot. \n",
    "<li> Our goal is to use X to predict Y, just as it was in linear regression. \n",
    "<li> However, there's not a very obvious way to use the X value only to do a linear regression that has any degree of accuracy.\n",
    "    <ul>\n",
    "    <li> Seriously, try to generate any line of best fit that doesn't have massive residuals. \n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BMI</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BMI  Outcome\n",
       "0  33.6        1\n",
       "1  26.6        0\n",
       "2  23.3        1\n",
       "3  28.1        0\n",
       "4  43.1        1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "df = df[[\"BMI\",\"Outcome\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
       "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
       "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
       "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
       "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   2     3       0  \n",
       "1   0     3       0  \n",
       "2   0     3       0  \n",
       "3   1     3       0  \n",
       "4   3     2       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load some data. We'll use this in a bit. \n",
    "df = pd.read_csv(\"data/heart.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `BMI` for parameter `x`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatterplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBMI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOutcome\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_2/lib/python3.10/site-packages/seaborn/relational.py:742\u001b[0m, in \u001b[0;36mscatterplot\u001b[0;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatterplot\u001b[39m(\n\u001b[1;32m    733\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    734\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    739\u001b[0m ):\n\u001b[1;32m    741\u001b[0m     variables \u001b[38;5;241m=\u001b[39m _ScatterPlotter\u001b[38;5;241m.\u001b[39mget_semantics(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m--> 742\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_ScatterPlotter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[1;32m    745\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_2/lib/python3.10/site-packages/seaborn/relational.py:538\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[0;34m(self, data, variables, legend)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{}, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    530\u001b[0m \n\u001b[1;32m    531\u001b[0m     \u001b[38;5;66;03m# TODO this is messy, we want the mapping to be agnostic about\u001b[39;00m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    535\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    536\u001b[0m     )\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend \u001b[38;5;241m=\u001b[39m legend\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_2/lib/python3.10/site-packages/seaborn/_oldcore.py:640\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semantic_mappings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    643\u001b[0m \n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# Create the mapping function\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     map_func \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmap, plotter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_2/lib/python3.10/site-packages/seaborn/_oldcore.py:701\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 701\u001b[0m     plot_data, variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables_longform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_data \u001b[38;5;241m=\u001b[39m plot_data\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;241m=\u001b[39m variables\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_2/lib/python3.10/site-packages/seaborn/_oldcore.py:938\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_longform\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m    934\u001b[0m \n\u001b[1;32m    935\u001b[0m     \u001b[38;5;66;03m# This looks like a column name but we don't know what it means!\u001b[39;00m\n\u001b[1;32m    937\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret value `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for parameter `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m \n\u001b[1;32m    942\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value is itself data\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[1;32m    944\u001b[0m     \u001b[38;5;66;03m# Raise when data object is present and a vector can't matched\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, pd\u001b[38;5;241m.\u001b[39mSeries):\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret value `BMI` for parameter `x`"
     ]
    }
   ],
   "source": [
    "sns.scatterplot(data=df, x=\"BMI\", y=\"Outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So What Do We Do?\n",
    "\n",
    "We need something that can transform our simple linear fitting into something... else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a way to transform our linear regression into something that can be used for classification. Simply put, when we perform a linear regression to predict a value, we have something that we can calculate directly using the least squares algorithm with a one-X calculation, or the equivalent matrix multiplication when we have multiple features. In other words, we can use the incoming data that we have, both the X and the Y, to calculate the coefficients that we need to predict Y.\n",
    "\n",
    "![Least Squares Equation](images/least_squares_equation.jpg \"Least Squares Equation\")\n",
    "\n",
    "However, when we are doing classification, we don't have a Y value that we can use to calculate the coefficients. We have a Y value that we want to predict, but we don't have a Y value that we can use to calculate the coefficients. There's no linear regression possible that will spit out yes or no, or 1 or 0. \n",
    "\n",
    "We can, however, use a few twists on the linear regression to get there, first we can change the linear regression equation from producing the actual value into an equation that produces the log odds, or the logit, of the value. So, the \"logistic\" part of the logistic regression is due to the logit (or the log-ness), and the \"linear\" part of the logistic regression is due to the fact that we are still using a linear equation to calculate the logit. This part is, from our perspective, arbitrary - someone figured out that this works, and it does, so we use it. So, when doing a logistic regression, we are doing a linear regression of the logit - or the log odds - of the value that we are trying to predict.\n",
    "\n",
    "<big>\n",
    "\n",
    "$ logit = log(odds) = log(\\frac{p}{(1-p)}) = log(\\frac{p}{not p}) = log(\\frac{prob(1)}{prob(0)}) = m*x + b$ \n",
    "\n",
    "</big>\n",
    "\n",
    "Where p is the probability of the event happening, and 1-p is the probability of the event not happening. We represent one of the two outcomes as p, or 1, and the other possible outcome as 1-p, or 0. Which one is which is arbitrary, but we need to be consistent. It is possible to do logistic regression with more than two outcomes, but that comes later, for now we are just doing two outcomes - a binary classification.\n",
    "\n",
    "At this point still have no way to use a regression to predict yes or no, but we do have a way to use a regression to predict the log odds of yes or no. The second step in making things work is to translate the log odds into probability. The two images below show a full derivation for a one-X scenario, if we have more than one input, we'd just have more than one X and more than one coefficient, or w in the example below:\n",
    "\n",
    "![Logistic 1](images/logit_math_1.jpeg \"Logistic 1\")\n",
    "![Logistic 2](images/logit_math_2.jpeg \"Logistic 2\")\n",
    "\n",
    "We don't need to delve super deep into this derivation, be able to do it, or understand all of the details to understand logistic regression well enough to do machine learning, but we do need to understand the basic idea.\n",
    "\n",
    "## Enter the Sigmoid\n",
    "\n",
    "Our logistic regression really picks up at one of the steps above, where a note takes care to point out that we have the sigmoid - a formula of the form:\n",
    "\n",
    "<big><big>\n",
    "\n",
    "$ p = \\frac{1}{1+e^{-z}} $\n",
    "\n",
    "</big></big>\n",
    "\n",
    "Also note that the z is the logit, or the outcome of the linear regression that we mentioned up above. The line below expresses it slightly differently, by representing the sigmoid as we would with a python function, that takes in a value for z (the logit) and returns a value for p (the probability). We can also say something like we take the outcome of our model, and pass it through the sigmoid function to get the probability of the outcome being 1. The sigmoid function has the fun property of being bounded between 0 and 1, which is exactly what we need for a probability. It has another fun property of being quite steep in the middle, meaning that it is \"decisive\" in splitting data to one side or another. \n",
    "\n",
    "![Sigmoid](images/sigmoid.png \"Sigmoid\")\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "This is where we have a really ML-relevant step - we can produce a prediction of the probability of 1/0 through a regression. That step of taking the regression output and passing it through the sigmoid function is the key to logistic regression. We can use the linear regression to calculate the logit, and then we can use the sigmoid function to turn the logit into a probability. We can then use the probability to make a classification prediction by relying on the shape of the sigmoid, and dividing the data at 0.5. As a note, this is also how a neural network works at its core. \n",
    "\n",
    "This means that after all those steps, for any given input, we can put the value in for X (or several Xs), calculate the logit, pass it through the sigmoid, perform a cutoff at 0.5, and make a prediction of either 1 or 0. Our full, end-to-end logistic regression looks like this:\n",
    "\n",
    "<big><big>\n",
    "\n",
    "$ p = \\frac{1}{1+e^{-(m*x+b)}} $\n",
    "\n",
    "</big></big>\n",
    "\n",
    "Where p is the probability of the outcome being 1, and m and b are the coefficients that we calculate using the least squares algorithm.\n",
    "\n",
    "## What About the Coefficients?\n",
    "\n",
    "One key thing that we've been missing all along is how to calculate the coefficients, a seemingly important step. In a logistic regression we don't have a way to directly calculate the slope(s) and intercept like we do with a linear regression. We can't use the least squares algorithm to calculate the coefficients, because we don't have a Y value to use - we only have probabilities. The probabilistic nature of the calculation we just looked at stops us from having certainty in our coefficients.\n",
    "\n",
    "To figure out the coefficients in a logistic regression we need a totally different approach, one of the most important concepts in machine learning - gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is a method of finding the minimum of a function - in our case it is a method of finding the minimum of the cost function. \n",
    "\n",
    "### The Cost Function\n",
    "\n",
    "Before we can proceed with gradient descent we must introduce the cost function - a function that we define that tells us how wrong our model is. This function is important because the idea of gradient descent is to minimize the cost function by running several iterations; at each stage the cost function tells the algorithm how wrong the model is, so it can adjust be more accurate. \n",
    "\n",
    "The cost function is a function of the actual values and the predicted values. In a regression, the cost function is the mean squared error, or the average of the squared residuals. We can measure how wrong our model is by calculating the cost function, if it gets lower the model is better, if it gets higher the model is worse. In a logistic regression, the idea of the \"cost\" is slightly different because we are predicting a yes/no, so the idea of \"slightly better\" doesn't apply in the same way as it does with MSE. In a classification we only have correct or incorrect, so if we have a value in the training data where the model is 50.1% sure that it is a 1, and the actual value is a 0, that's just as wrong as if the model was 99.9% sure that it was a 1.\n",
    "\n",
    "In a classification problem we need to look at the cost differently, not as a function of the final prediction, but as a function of the probabilities. The output of the sigmoid function is a probability, so we can use that probability to calculate the cost. In this case, if we have a value that is a 1, and the model predicts a 0.9, the cost, or loss is low as we only \"missed\" by .1. If we have a value that is a 1, and the model predicts a 0.1, the cost, or loss is high as we \"missed\" by .9. This allows the model to learn more accurately how good it is, and when we continue with the gradient descent, that is critical. There are several possible cost functions, but the most common is the log loss function, or binary cross entropy:\n",
    "\n",
    "<big><big>\n",
    "\n",
    "$ logloss = 1/n \\sum_{i=1}^{n} -y_i*log(\\hat{y_i}) - (1-y_i)*log(1-\\hat{y_i}) $\n",
    "\n",
    "</big></big>\n",
    "\n",
    "So, every time we calculate the cost of our model, we are calculating the log loss, which will not tell us how many we get right or wrong, but will tell us how wrong we are in aggregate - just like the MSE in a regression.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Back to gradient descent. The basic idea of gradient descent is:\n",
    "\n",
    "<ul>\n",
    "<li> Start with a random guess for the coefficients. </li>\n",
    "<li> Calculate the cost function. </li>\n",
    "<li> Adjust the coefficients to make the cost function lower. </li>\n",
    "<li> Repeat until the cost function is as low as possible. </li>\n",
    "</ul>\n",
    "\n",
    "Or basically an elaborate, math nerd version of \"guess and test\". The core idea of gradient descent is that simple - it is an algorithm that adjusts coefficients, measures cost, and repeats until the cost is as low as possible. \n",
    "\n",
    "The one step that is a little more complicated is the \"adjust the coefficients to make the cost function lower\" step. It isn't clear how the algorithm knows to make the coefficients lower or higher, and by how much. The answer to this lies in calculus, and the idea of a derivative. The derivative of a function is the slope of the function at a given point. So, if we calculate the derivative of the loss function with the coefficients at a certain point, that will tell us the slope of the loss function at that point. If the slope is positive, we need to move the coefficients down, if the slope is negative, we need to move the coefficients up. The size of the slope tells us how much to move the coefficients. This leads to the most common visualization of gradient descent, the \"hill climbing\" visualization:\n",
    "\n",
    "![Gradient Descent](images/gradient_descent.png \"Gradient Descent\")\n",
    "\n",
    "So in each round of the process, the algorithm will:\n",
    "\n",
    "<ul>\n",
    "<li> Calculate the cost function. </li>\n",
    "<li> Calculate the derivative of the cost function.</li> \n",
    "    <ul>\n",
    "    <li> Specifically, the partial derivative of the cost function with respect to each coefficient. </li>\n",
    "    <li> This means that each separate derivative will tell us how much to adjust each coefficient individually, they each have their own \"slope\". </li>\n",
    "    </ul>\n",
    "<li> Adjust the coefficients by the derivative times the learning rate. </li>\n",
    "    <ul>\n",
    "    <li> The learning rate is a hyperparameter that we set, it is the size of the step that we take in each iteration. </li>\n",
    "    <li> If the learning rate is too small, the algorithm will take a long time to converge, or find an answer. </li>\n",
    "    <li> If the learning rate is too large, the algorithm will overshoot the answer and never converge. </li>\n",
    "    <li> We will look more at learning rates in the future. </li>\n",
    "    </ul>\n",
    "<li> Repeat until the cost function is as low as possible. </li>\n",
    "    <ul>\n",
    "    <li> The cost is low when the derivitive is zero, or when the slope is zero. This means that the algorithm is at the bottom of the cost. </li>\n",
    "    <li> The algorithm will never actually reach zero, but it will get close enough to be good enough. </li>\n",
    "    <li> We often also set a cutoff, such as maximum number of iterations, to prevent the algorithm from running forever. </li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "Critically here, as the gradient descent process is working, we don't look at correct or incorrect predictions at all, we only look that the cost - or the measure of how close our prediction was to the correct answer. Our model making correct predictions will be highly correlated with the cost, but it is not a factor in the gradient descent process. We only evaluate the actual accuracy in predictions after the model is created. \n",
    "\n",
    "#### Extra Details\n",
    "\n",
    "The gradient descent algorithm is a little more complicated than that, but not much. The algorithm is:\n",
    "\n",
    "<big><big>\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1) $\n",
    "\n",
    "</big></big>\n",
    "\n",
    "Where $\\theta_j$ is the coefficient that we are adjusting, $\\alpha$ is the learning rate, and $J(\\theta_0, \\theta_1)$ is the cost function. The learning rate is a hyperparameter that we set, and the cost function is the function that we are trying to minimize. The algorithm is a little more complicated than that, but not much. The algorithm is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough Example\n",
    "\n",
    "We'll walk through the first part of a logistic regression here, through the first round of gradient descent. Simple example... We want to predict Y, given some values of X. For this, we can say that the values are as follows:\n",
    "\n",
    "<ul>\n",
    "<li> Y = Passed high school. 1 = Yes.\n",
    "<li> X1 = Attended class. 1 = Yes.\n",
    "<li> X2 = Studied at home. 1 = Yes. \n",
    "</ul>\n",
    "\n",
    "Each set of values (a column) is one person, so we have two people who passed and two who did not. The details of the data don't matter much, we're looking at the mechanics here. We'll do a real one in a min. This middle part of the curve is sometimes called the Decision Boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "y = np.array([0, 1, 0, 1])\n",
    "x1 = np.array([0, 0, 0, 1])\n",
    "x2 = np.array([0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a regression, using the logit formula:\n",
    "\n",
    "$\\log o = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 $\n",
    "\n",
    "We don't know our coefficients though - the process for determining them isn't a direct calculation like linear regression. Here we need to try some, check our error, then improve. (This is a common thing in ML).\n",
    "\n",
    "For this, we are making an arbitrary guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [-1.5, 2.8, 1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate it out, just like a linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5, -0.4, -0.4,  2.4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_o = b[0] + b[1] * x1 + b[2] * x2\n",
    "log_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert log odds to odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22313016,  0.67032005,  0.67032005, 11.02317638])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = np.exp(log_o)\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert odds to probabilities. These probabilities are the outputs of the sigmoid calculation, and we can use them to classify by just labeling things that are over the cutoff (usually .5) as 1s and the things that are under as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18242552, 0.40131234, 0.40131234, 0.9168273 ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = o / (o+1)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Real Usage\n",
    "\n",
    "Those are all the predicted probabilities of each person passing high school. \n",
    "\n",
    "Now, a little weirdness. We started out this whole thing with some pretty random values for all the coefficients, so why would we trust these predictions? Well, right now, we wouldn't. What we need to do to make a model that is actually accurate is to check how well we did now, then make some improvements. \n",
    "\n",
    "To check how well we did now, we can calculate how close the probabilites are to the real values. E.g. Person #4 did really graduate, and our model predicted a ~92% chance of them graduating, that's good. Person #2 also graduated, but our model only predicted a ~40% chance of them graduating, that's bad. So our metric for evaluating is that we want our predictions to be as close as possible to the real values - or we want \"1\"s to have high percentages, and \"0\"s to have low percentages. The more sharpely we can discriminate between passes and fails, the more accurate the model. \n",
    "\n",
    "We can calculate this overall accuracy pretty simply - how likely are we to predict the correct answer? \n",
    "\n",
    "### Cost and Loss\n",
    "\n",
    "Some new concpets that are introduced here, and are important going forward, are the ideas of cost and loss. When doing these types of iteritive training processes our progress is tracked by our loss, or the amount of error. This amount is calculated by our loss function, or how that error is calculated. This is directly comparable to the MSE/RMSE process we looked at previously, we have some calculation to determine our overall accuracy. \n",
    "\n",
    "Each time we do an iteration, we get some amount of error, or loss. The best solution is where this loss is at it's lowest. This is the same idea as how the best linear regression model is best when the loss - the linear least squares distance - is at it's lowest. \n",
    "\n",
    "<b>Note:</b> the terms cost and loss are often used interchangably, and this is generally fine. Technically the loss function is for each specific example, and the cost is the overall summary. In practice, it is not that big of a deal to swap the terms - there won't really be many, if any, scenarios in which that will be confusing. \n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is the way that many algorithms try to minimize their loss. In short, this process is just:\n",
    "<ul>\n",
    "<li> Create a model. \n",
    "<li> Measure the loss. \n",
    "<li> Adjust the model's values (i.e. the slopes and intercepts here)\n",
    "<li> Check the loss again. \n",
    "<li> Repeat until you reach the lowest value for loss. (i.e. the most accurate model)\n",
    "</ul>\n",
    "\n",
    "![Gradient](images/grad_desc.png \"Gradient\")\n",
    "\n",
    "The algorithm \"knows which way to go\" in adjusting the weights between each trial via some calculus and matrix math that we will peek into when we look at neural networks. The algorithm can basically use partial derivitives to attribute error to the different values (the slopes), as well as if they are too high or too low. Each step moves these values a little, then we recheck. \n",
    "\n",
    "We will explore the details of gradient descent much more as we get into the machine learning stuff, for now understanding the general idea is good enough. This process is how most machine learning models \"learn\", and this is what is going on when they are processing for a long time. Each iteration moves the results (hopefully) to a point where the model has a little less error, and eventually we either \"find the bottom\" - which in logistic regression is the slopes and intercept values for the log-odds regression above, or we hit a limit of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is our Loss?\n",
    "\n",
    "Our loss in the example above is a calculation the summarizes all of our individual errors. In the previous cell we calculated the probability of each person passing (being 1), our original Y data shows us the true probability of each person passing (either 0 or 1). Each prediction has an error of the distance between that true value and our expected probability. \n",
    "\n",
    "E.g. for the second item, this person passed, so the real value is 1. We predicted a ~40% likelihood of them passing, so our error there is ~60%. Person 4 passed, we predicted a ~92% chance of them passing, so our error is ~8%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18242552, 0.59868766, 0.40131234, 0.0831727 ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#likes = np.where(y, p, 1-p)\n",
    "likes = np.where(y, 1-p, p)\n",
    "likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function\n",
    "\n",
    "These individual accuracies can be tallied up, we'll do a simple one with a common loss function called log-loss. There are a bunch of \"real\" loss functions that we can use, we'll explore them later on in machine learning. The most simple one is also based on the log of the odds, it is called Binary Cross-entropy, or Log-Loss. Don't worry about these details too much now, we will explore this later. \n",
    "\n",
    "![Log Loss](images/log_loss.png \"Log Loss\")\n",
    "\n",
    "The goal of the algorithm is to find the smallest value for this totalled loss, that's when we are most accurate overall. \n",
    "\n",
    "We can turn this loss total into an overall cost by just dividing by n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42856998373415184"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_sum = 0\n",
    "for i in range(len(p)):\n",
    "    if y[i] == 1:\n",
    "        loss_sum += -np.log(p[i])\n",
    "    elif y[i] == 0:\n",
    "        loss_sum += -np.log(1-p[i])\n",
    "cost = loss_sum/len(p)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall goal is to find the lowest possibility for this value. The lower this value, the closer to reality our model was predicting, the more accurate we can be. The process is to now to:\n",
    "<ul>\n",
    "<li> Take this amount of overall error, use it to make an adjustment to those starting values. (The ones we made up to start)\n",
    "    <ul>\n",
    "    <li> This step is something we'll look at in more depth with neural networks. In involves some partial derivitives which (kind of) allow us to work backwards and attribue parts of the errors to the original inputs. \n",
    "    </ul>\n",
    "<li> Calculate the new error with the different starting point. \n",
    "<li> Repeat - each stage should move us a little closer to the \"true\" answer. \n",
    "    <ul>\n",
    "    <li> In other words, we are repeating the process over and over until we've found the solution that minimizes our overal cost/loss (the amount of error). \n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "This process is called Gradient Descent and is something we will care about more in ML class. Basically we define something called a loss function, which measures how much error we have. We then repeat a bunch of trials with different coef values, and measure the loss each time. We keep repeating until we've found the lowest amount of loss - or the smallest amount of error. The math can be complex, but the idea is pretty simple. If we manually changed the array of b values, ran the model, collected the LIKE value, and finally selected the combination with the best LIKE, that'd be a crude version of the same thing. This idea is common later on. Here, sklearn or statsmodels do it for us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Gradient Descent Process in Practice\n",
    "\n",
    "![Logistic](images/logistic.webp \"Logistic\")\n",
    "\n",
    "As we use logistic regression, the fit() step that trains our model is the first look at a model that has a substantial training step - the numerous rounds of gradient descent to find the best coefficients. The process looks like this with the heart disease dataset:\n",
    "<ul>\n",
    "<li> Create the linear regression, starting with random coefficients for slopes and intercept. </li>\n",
    "<li> Calculate the result of the linear regression, the log odds, for each row. </li>\n",
    "<li> Pass the log odds through the sigmoid function to get the probability of each row. </li>\n",
    "<li> Calculate the loss, or the error, of the model. Normally by measuring the \"true\" value of 1 or 0, with the predicted probability of a 1 that came out of the sigmoid. </li>\n",
    "<li> Calculate the derivative of the loss function with respect to each coefficient. </li>\n",
    "<li> Adjust the coefficients by the derivative times the learning rate. </li>\n",
    "<li> Go back up to calculating the result of the linear regression, and repeat until the loss is as low as possible or we hit a limit. </li>\n",
    "<li> The coefficients at the stopping point are the coefficients that complete our model. </li>\n",
    "</ul>\n",
    "\n",
    "This process is totally different in concept to the linear regressions that we've looked at that can be calculated directly through least squares. This process is iterative, and requires a lot of computation, which is a main reason why we need lots of processing power for machine learning. In real applications, most algorithms, even linear regression (more details later), use some form of gradient descent to find the best coefficients. This approach is very useful and versatile as we can always work towards a solution incrementally, almost no matter where we start. As long as we can define some measure of cost, or some measure that tells the model how well it is doing, we can use gradient descent to push the model to be better. \n",
    "\n",
    "Gradient descent is also the core of neural networks, in fact a neural network is roughly like a series of many large logistic regressions, each one feeding into the next. The process of training a neural network is very similar to this, the main difference being that neural networks generally have far more weights that can be adjusted, giving them the ability to learn very complex patterns in data, provided we have enough data and enough processing power for it to be trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sigmoid and the Regression\n",
    "\n",
    "The sigmoid function is a function that takes in inputs (X values) and squishes all the outputs (Y values) between 0 and 1. The sigmoid is also the inverse of the logit function. The function is:\n",
    "\n",
    "$ g(x) = \\frac{1}{(1+e^-x)} = logit^-1 $\n",
    "\n",
    "A graph of what it ends up looking like is below. (Ignore the red line for now). The important part is now we have a way to connect the probabilities to our sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAGsCAYAAAAVEdLDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEJklEQVR4nO3deXRUhf3+8Wdmkkz2gSwEAgmEPeySAALibhStda2oLYiClbpUjFpF/VXr12/TWrXWBdSvoOJWXOrW4oKtRTYVQtg3WQNZCElgJutkMnN/fwSiEdAEktyZyft1zj2TuXOveXLOOMnDvfdzLYZhGAIAAACAIGI1OwAAAAAAtDaKDgAAAICgQ9EBAAAAEHQoOgAAAACCDkUHAAAAQNCh6AAAAAAIOhQdAAAAAEEnxOwAzeHz+VRYWKiYmBhZLBaz4wAAAAAwiWEYqqioUHJysqzW4x+3CYiiU1hYqJSUFLNjAAAAAPATe/fuVY8ePY77ekAUnZiYGEkNP0xsbKzJaQAAAACYxeVyKSUlpbEjHE9AFJ0jp6vFxsZSdAAAAAD85CUtDCMAAAAAEHQoOgAAAACCDkUHAAAAQNCh6AAAAAAIOhQdAAAAAEGHogMAAAAg6FB0AAAAAAQdig4AAACAoEPRAQAAABB0Wlx0vvzyS1188cVKTk6WxWLR+++//5P7LF68WBkZGQoPD1fv3r313HPPnUhWAAAAAGiWFhedqqoqDR8+XM8880yztt+1a5cuvPBCTZgwQXl5ebrvvvv029/+Vu+++26LwwIAAABAc4S0dIeJEydq4sSJzd7+ueeeU2pqqp588klJUnp6ulatWqXHHntMV1xxxTH3cbvdcrvdjc9dLldLYwIAAADowNr8Gp0VK1YoKyurybrzzz9fq1atksfjOeY+OTk5cjgcjUtKSkpbxwQAAAAQRNq86BQXFyspKanJuqSkJNXX16u0tPSY+8yaNUtOp7Nx2bt3b1vHBAAAADo0wzBU6/HqYFWdipw12nmgUhsLncrdc1DLtpdqU2FgnWXV4lPXToTFYmny3DCMY64/wm63y263t3kuAAAAIJAYhqEaj1dVbq+q6+pV6a5XdZ1XVe56Vbm9qqqrV7W7XlWH11XXeVXprleNxyu3x6saj1c1dV7VenyqPfz8u0ffj37vnw9P1lPXnNJOP+nJa/Oi07VrVxUXFzdZV1JSopCQEMXHx7f1twcAAAD8gsfrU0VtvVw1nobHWo8qaj1y1TR87frBa64az+Hi4v2u0NTV6/AxgzYVarMoPNSm8FCbIg4vXR3hbf+NW1GbF52xY8fqo48+arLus88+U2ZmpkJDQ9v62wMAAACtyuP16WB1nQ5Ve1ReVadD1XU6WO055jpXjedwaWk4qtKaosJsirSHKCrMpih7iKLCQhRpP/K1TZFhIYq2hygizKaosMOlJcwme0jDY0SoTeGh1sOP370eHmJViC3wb7fZ4qJTWVmp7du3Nz7ftWuX1qxZo7i4OKWmpmrWrFkqKCjQ/PnzJUkzZszQM888o+zsbN14441asWKF5s6dqzfffLP1fgoAAADgBHl9hg5W1+lAhVullYeXijqVVrob1jUWlzodrPKo0l1/Ut8vKsymmPBQxUaEKDY8VDHhIYqNOPwYHtr4Wkx4qGLsIYqyhyjySJmx2xQVFqKIUJus1mNfBoIGLS46q1at0llnndX4PDs7W5J03XXX6eWXX1ZRUZHy8/MbX09LS9PChQt1xx136Nlnn1VycrKeeuqp446WBgAAAFqDx+tTSYVbxc4aFTlrVeysVUmFW6UVbh04UmIq61Re5ZavhaeDWSxSp4hQdY4MU+eoMHWODFWnyO8/NnztiAxVbPjhJaLhCEswHC0JBBbDaI+z/E6Oy+WSw+GQ0+lUbGys2XEAAABgMne9V0WHalV4qEbFrloVOWu1//BjsbNWxa5alVa6m309i8UidY4MU2K0XQkxYUqIth/+2q74qDDFRYWpU2TDY+fD5YUjKuZobjdol6lrAAAAQEt4vD4VHarVvoPV2newRnsPP+47WK295TXaX1HbrBITarMoKTZc3RzhSoptWBJj7EqItishOkyJMQ2FJi4qjCMtQYaiAwAAAFPUerzKL6/WrtIq7Sqt0u7Dj/sO1qjIWfOTp5OFh1qV3ClC3Rzh6hoboa4Ou7o6ItQtNlxdHQ1LXGQYR146KIoOAAAA2ozPZ2jfwRrtKK3UrgOHC01ZlXYeqFKhs+ZHj8qEhVjVo1OEesRFqkfnCKV0bnjs0TlCKXGRio8KO+59GQGKDgAAAE6az2eo4FCNvi2p0Lb9ldq2v0Lf7q/UtyUVP3ojyhh7iHolRCktIerwY6RS4yKV0jlSCdF2jsbghFF0AAAA0CLOao82Fjm1qdClLcUV+nZ/hb4tqVR13bHvExMWYlVafEOZSUuMavg6MUq94qOUEM1RGbQNig4AAACOyTAMFbtqtbHApY2FLm0sdGpjoUsFh2qOuX2ozaLeCdHqlxSt/kkxh5dopcZFcqE/2h1FBwAAAJKk0kq31uQf0pq9h7R23yFtLHSpvKrumNv26ByhwcmxSu8W21hoesZHKZRCAz9B0QEAAOiAaj1ebSx0Ku9wsVmz95D2HTz6SI3NalHfxGgNTo7VoMPL4G4OOSJDTUgNNB9FBwAAoAMoq3Rr5e6D+mZXuVbtKdemQpfqfzC/2WKR+iRGa0RKJ41I6aSh3R0a0DVG4aE2k1IDJ46iAwAAEIQKD9Vo5e5yfb2rXN/sKtf2ksqjtkmIDmssNSNSOmtYikOx4RypQXCg6AAAAASBkopaLd9epqXbS/XVzrJjnobWPylao9PiNKpXnEamdlaPzhFMPEPQougAAAAEoOq6en2zq1xLvy3V0u2l2lJc0eR1m9WiwcmxGt0rrrHcdI4KMykt0P4oOgAAAAHAMAxtLqrQF1tLtOTbA1q955DqvE1vxDmke6zG903QuD4JyujZWdF2/tRDx8W7HwAAwE/VerxavqNU/95cov9sKVGRs7bJ6907RWhCvwSN79uwxHHEBmhE0QEAAPAjRc4a/WdLif6zuUTLdpSq1vPdUZvwUKtO65ugM/on6rR+ieoVH8k1NsBxUHQAAABMll9WrY83FOnjDcVas/dQk9e6d4rQ2QO76Oz0LhrbO55Rz0AzUXQAAABMsONApT5e31BuNha6GtdbLNIpKZ10TnqSzknvogFJMRy1AU4ARQcAAKCd7Cmr0gdrCvXPdYXatv+7+9rYrBad2jtOFwzppvMHJ6lLTLiJKYHgQNEBAABoQ2WVbv1rfZHeyytQXv6hxvWhNovG9UnQhUO76rxBXRkkALQyig4AAEArq6nz6rNNxfpgTaG+3HZA9T5DkmS1SOP7Jujnw5OVNairHJGhJicFghdFBwAAoBUYhqG1+5xasDJfH64pVFWdt/G1od0dumREsn4+PFldYjktDWgPFB0AAICTcLCqTu/lFeitVXu1pbiicX1KXIQuHdFdl4zorr5dok1MCHRMFB0AAIAW8vkMrdhZpr+v3KtPNxSrzttwrxt7iFUXDu2mSaNSNCYtjmlpgIkoOgAAAM1UUevRu7n7NP+rPdp5oKpx/aBusbpmdIp+PqK7HBFcdwP4A4oOAADAT9heUqH5K/bo3dx9jdfeRNtDdOkpybp6VKqGdHeYnBDAD1F0AAAAjsHrM/SfLSV6ZfluLd1e2ri+b5doXTe2py4b2UPRdv6UAvwV/3cCAAB8T63Hq3dX79OLS3ZpV2nD6WlWi3ROepKmjuulcX3iufYGCAAUHQAAADVMT3v1qz16ZflulVXVSZJiw0N0zZhU/WpMT6XERZqcEEBLUHQAAECHtre8WnOX7tKClXtV42m4/qZ7pwhNOy1Nk0alKIrT04CAxP+5AACgQ9pVWqVn/rNd768pkNdnSGqYnnbTGb114dBuCrVZTU4I4GRQdAAAQIey40Clnj1ccA73G03ol6CbTu+j8X25/gYIFhQdAADQIWwvqdDT/9muj9YWNhacswd20W/P6acRKZ1MzQag9VF0AABAUNtdWqUnFm3TR+sKZRwuOOemJ+n2c/ppaA/ufwMEK4oOAAAISvtdtXrq399qwcq9qj98COf8wUm67ex+3OAT6AAoOgAAIKg4qz167ssdemnZLtV6fJIaTlG7K2uABiXHmpwOQHuh6AAAgKBQ6/HqpWW7Nee/2+WqrZckZfTsrHsuGKjRaXEmpwPQ3ig6AAAgoBmGoQ/XFurPH29RobNWkjQgKUZ3nz9A56R3YYoa0EFRdAAAQMDKyz+o//nnJq3OPyRJSnaE667zB+iSEd1ls1JwgI6MogMAAAJOkbNGj36yVe/lFUiSIkJtuvnMPrrx9N4KD7WZnA6AP6DoAACAgFHr8er5xTs1Z/H2xkEDV4zsod9dMEBJseEmpwPgTyg6AAAgIPx3a4ke/HCj9pRVS5Iye3bW7y8epGE9OpkbDIBfougAAAC/VnioRv/zz036eEOxJCkp1q4HLhqknw3rxqABAMdF0QEAAH7J4/XppWW79OTn36q6ziub1aLrx/XSzPP6K9rOnzAAfhyfEgAAwO/k7jmo+/6xXlv3V0hquB/OI5cOUXo3bvgJoHkoOgAAwG9Uuev1l0+36pUVu2UYUufIUM26MF1XjuwhK+OiAbQARQcAAPiFJd8e0Kx/rNe+gzWSpCszeuj+C9PVOSrM5GQAAhFFBwAAmMpZ7dH/Ltykt1btkyR17xShnMuH6vT+iSYnAxDIKDoAAMA0n20s1v3vb9CBCrcsFum6sb109/kDFMWwAQAniU8RAADQ7ly1Hv3hw016d3XDUZzeiVF69IphyuwVZ3IyAMGCogMAANrVih1luuvttSo4VCOrRfr16X0089x+Cg+1mR0NQBCh6AAAgHZR6/HqsU+3au6yXTIMKTUuUk9cNZyjOADaBEUHAAC0uQ0FTmW/tUbb9ldKkq4elaIHfjaIG38CaDN8ugAAgDbj8xmau3SXHv10izxeQwnRYfrT5cN07qAks6MBCHIUHQAA0CbKKt268+21+u/WA5Kk8wYl6U+XD1V8tN3kZAA6AooOAABodct3lGrm39eopMIte4hVv794kK4dnSqLxWJ2NAAdBEUHAAC0mnqvT0/9+1s9/cV2GYbUt0u0nrn2FA3sGmt2NAAdDEUHAAC0iiJnjW5/c42+2V0uSZqUmaIHfz5IkWH8uQGg/fHJAwAATtrSb0t125urdbDao2h7iP73siG6ZER3s2MB6MAoOgAA4IT5fIbmLN6hxz/bKp8hDekeq2euGaleCVFmRwPQwVF0AADACXHWeHTnW2v0+eYSSQ2nqv3hksEKD7WZnAwAJOuJ7DR79mylpaUpPDxcGRkZWrJkyY9u//rrr2v48OGKjIxUt27ddP3116usrOyEAgMAAPNtKnTp588s1eebSxQWYtWfrxiqP185jJIDwG+0uOgsWLBAM2fO1P3336+8vDxNmDBBEydOVH5+/jG3X7p0qaZMmaJp06Zp48aNevvtt7Vy5UpNnz79pMMDAID2927uPl0+Z5n2lFWre6cIvTtjnCaNSjU7FgA0YTEMw2jJDmPGjNHIkSM1Z86cxnXp6em69NJLlZOTc9T2jz32mObMmaMdO3Y0rnv66af16KOPau/evcf8Hm63W263u/G5y+VSSkqKnE6nYmMZTwkAgBnqvT7978LNemnZbknSGf0T9eSkEeocFWZuMAAdisvlksPh+Mlu0KIjOnV1dcrNzVVWVlaT9VlZWVq+fPkx9xk3bpz27dunhQsXyjAM7d+/X++8844uuuii436fnJwcORyOxiUlJaUlMQEAQCtzVnt0/csrG0vOb8/pp3lTR1FyAPitFhWd0tJSeb1eJSUlNVmflJSk4uLiY+4zbtw4vf7665o0aZLCwsLUtWtXderUSU8//fRxv8+sWbPkdDobl+Md+QEAAG1ve0mlLp29TEu+LVVEqE1zfjlS2ef1l81qMTsaABzXCQ0jsFiafrAZhnHUuiM2bdqk3/72t/r973+v3NxcffLJJ9q1a5dmzJhx3P++3W5XbGxskwUAALS//24t0WWzl2lXaZW6d4rQO78Zq4lDu5kdCwB+UovGSyckJMhmsx119KakpOSoozxH5OTkaPz48br77rslScOGDVNUVJQmTJigRx55RN268WEJAIC/MQxDc5fu0h8XbpbPkDJ7dtZzkzOUEG03OxoANEuLjuiEhYUpIyNDixYtarJ+0aJFGjdu3DH3qa6ultXa9NvYbA2jJ1s4BwEAALQDj9ene95dp0f+1VByrsrsoddvHEPJARBQWnzD0OzsbE2ePFmZmZkaO3asXnjhBeXn5zeeijZr1iwVFBRo/vz5kqSLL75YN954o+bMmaPzzz9fRUVFmjlzpkaPHq3k5OTW/WkAAMBJcdV6dPNrq7V0e6msFumBiwbp+vG9jnuKOgD4qxYXnUmTJqmsrEwPP/ywioqKNGTIEC1cuFA9e/aUJBUVFTW5p87UqVNVUVGhZ555Rnfeeac6deqks88+W3/+859b76cAAAAnreBQja5/6Rtt21+pyDCbnr12pM4a2MXsWABwQlp8Hx0zNHdWNgAAODEbCpy64eWVKqlwq0uMXfOmjtKQ7g6zYwHAUZrbDVp8RAcAAASX/2zZr1vfyFN1nVcDkmL00vWjlNwpwuxYAHBSKDoAAHRgr361Rw9+sEE+Q5rQL0HP/nKkYsNDzY4FACeNogMAQAdkGIYe+2yrnv1ih6SGyWr/e9lQhdpO6BZ7AOB3KDoAAHQw9V6fHnh/g/6+cq8k6c7z+uvWs/syWQ1AUKHoAADQgdR6vPrtm3n6bNN+WS3SHy8bqqtHp5odCwBaHUUHAIAOwlXr0Y2vrNLXu8oVFmLV09ecovMHdzU7FgC0CYoOAAAdQElFra6bt1Kbi1yKsYfo/67L1Km9482OBQBthqIDAECQ21NWpclzv1F+ebUSou165YZRGpzMPXIABDeKDgAAQWxrcYV++eLXKq10KzUuUq9OG62e8VFmxwKANkfRAQAgSK3f59TkeV/rULVHA7vGaP4No9UlNtzsWADQLig6AAAEodw95Zo6b6Uq3PUantJJ868fLUckNwIF0HFQdAAACDLLt5dq+vxVqq7zanSvOM2dmqmYcEoOgI6FogMAQBD5YmuJZryaK3e9TxP6Jej5yRmKDOPXPYCOh08+AACCxCcbinXbm6vl8Ro6N72Lnrl2pMJDbWbHAgBTUHQAAAgCH6wpUPZba+X1GbpoaDc9efUIhdqsZscCANNQdAAACHDv5xUo+6018hnS5SO769ErhimEkgOgg6PoAAAQwBqO5DSUnGtGp+h/Lx0qq9VidiwAMB3/3AMAQID6YE2B7ljQUHKuHkXJAYDvo+gAABCAvl9yJmWm6I+XUXIA4PsoOgAABJgP1xY2lpyrMnso53JKDgD8EEUHAIAA8tHaQs38e558hvSLjB760+XDKDkAcAwUHQAAAsQ/1xVq5uEjOVdm9NCfr6DkAMDxUHQAAAgAn20s1u1/XyOvz9AVIyk5APBTKDoAAPi5Jd8e0K1v5MnrM3TZKd316JXDZKPkAMCPougAAODHvtlVrhvnr1Kd16cLBnfVXyg5ANAsFB0AAPzUun2HdMPLK1Xr8enMAYl66ppTFGLjVzcANAeflgAA+KEtxS5NmfeNKt31OrV3nJ77VYbCQvi1DQDNxScmAAB+ZueBSv3qxW90qNqjESmd9OJ1oxQeajM7FgAEFIoOAAB+ZN/Bav3qxa9VWulWerdYvXL9aEXbQ8yOBQABh6IDAICfKHHV6pcvfq1CZ636JEbp1Wmj5YgMNTsWAAQkig4AAH7AWePRlHnfaE9ZtVLiIvT69FOVEG03OxYABCyKDgAAJqv1eDX9lZXaUlyhxBi7Xp92qro6ws2OBQABjaIDAICJ6r0+3frGaq3cfVAx4SGaf8NopcZHmh0LAAIeRQcAAJMYhqF7/7Fen28ukT3EqrnXjVJ6t1izYwFAUKDoAABgkj99skXv5O6TzWrRM9eO1Oi0OLMjAUDQoOgAAGCCF77coecX75Qk5Vw+VOcNSjI5EQAEF4oOAADt7J3cffrjwi2SpHsnDtRVmSkmJwKA4EPRAQCgHf17837d8+46SdKNE9J00+m9TU4EAMGJogMAQDvJ3XNQN7++Wl6foStG9tCsiemyWCxmxwKAoETRAQCgHew8UKnpr6yUu96nswd20Z+uGCqrlZIDAG2FogMAQBsrrXRr6ksrdbDao2E9HHrm2lMUauNXMAC0JT5lAQBoQ9V19Zr2yirll1crJS5Cc68bpciwELNjAUDQo+gAANBGvD5Dv31zjdbuPaROkaF6+frRSoyxmx0LADoEig4AAG3AMAw9+OEGfb55v8JCrHpxSqb6JEabHQsAOgyKDgAAbeC5xTv12lf5slikv00aocxecWZHAoAOhaIDAEAr+2BNgf78ScMNQR+4aJAmDu1mciIA6HgoOgAAtKIVO8p019trJUk3jE/TtNPSTE4EAB0TRQcAgFaybX+Ffv3qKnm8hi4c2lUPXJRudiQA6LAoOgAAtIIDFW5d/9JKVdTWK7NnZz1x1QhuCAoAJqLoAABwkmo9Xv361VUqOFSjnvGRemFKpsJDbWbHAoAOjaIDAMBJMAxDd7+zTnn5hxQbHqJ5U0cpLirM7FgA0OFRdAAAOAl//fxbfbS2UCFWi56bnMG9cgDAT1B0AAA4Qe/l7dNT//5WkvTHy4ZqXJ8EkxMBAI6g6AAAcAJW7i7XPe+slyTddEZvXTUqxeREAIDvo+gAANBCe8qqdNOruarz+nTB4K665/yBZkcCAPwARQcAgBZw1nh0w8srVV5Vp6HdHfrrJMZIA4A/ougAANBMHq9PN7+eqx0HqtTNEa4Xr8tURBhjpAHAH1F0AABoBsMw9PsPNmjZ9jJFhtk097pRSooNNzsWAOA4KDoAADTD3KW79OY3e2W1SE9fc4oGJceaHQkA8CMoOgAA/IQvtpbojws3S5Luv2iQzklPMjkRAOCnnFDRmT17ttLS0hQeHq6MjAwtWbLkR7d3u926//771bNnT9ntdvXp00fz5s07ocAAALSn7SUV+u0befIZ0tWjUnTD+F5mRwIANENIS3dYsGCBZs6cqdmzZ2v8+PF6/vnnNXHiRG3atEmpqanH3Oeqq67S/v37NXfuXPXt21clJSWqr68/6fAAALQlZ7VH019ZpQp3vUb3itPDlwyRxcKENQAIBBbDMIyW7DBmzBiNHDlSc+bMaVyXnp6uSy+9VDk5OUdt/8knn+jqq6/Wzp07FRcX16zv4Xa75Xa7G5+7XC6lpKTI6XQqNpZzogEAba/e69PUl1Zq6fZSde8UoQ9vHa/4aLvZsQCgw3O5XHI4HD/ZDVp06lpdXZ1yc3OVlZXVZH1WVpaWL19+zH0+/PBDZWZm6tFHH1X37t3Vv39/3XXXXaqpqTnu98nJyZHD4WhcUlK42zQAoH098q/NWrq9VJFhNv3flExKDgAEmBadulZaWiqv16ukpKYXYSYlJam4uPiY++zcuVNLly5VeHi43nvvPZWWlurmm29WeXn5ca/TmTVrlrKzsxufHzmiAwBAe/j7N/l6efluSdITV41gwhoABKAWX6Mj6ajzkw3DOO45yz6fTxaLRa+//rocDock6YknntCVV16pZ599VhEREUftY7fbZbfzL2cAgPb3za5y/b8PNkiSss/rrwuGdDU5EQDgRLTo1LWEhATZbLajjt6UlJQcdZTniG7duql79+6NJUdquKbHMAzt27fvBCIDANA29h2s1ozXcuXxGrpoWDfddnZfsyMBAE5Qi4pOWFiYMjIytGjRoibrFy1apHHjxh1zn/Hjx6uwsFCVlZWN67Zt2yar1aoePXqcQGQAAFpflbte019ZpfKqOg3pHqvHrhzOhDUACGAtvo9Odna2XnzxRc2bN0+bN2/WHXfcofz8fM2YMUNSw/U1U6ZMadz+2muvVXx8vK6//npt2rRJX375pe6++27dcMMNxzxtDQCA9ubzGcp+a422FFcoIdquFyZnKiLMZnYsAMBJaPE1OpMmTVJZWZkefvhhFRUVaciQIVq4cKF69uwpSSoqKlJ+fn7j9tHR0Vq0aJFuu+02ZWZmKj4+XldddZUeeeSR1vspAAA4CU9+vk2fbtyvMJtVz0/OUHIn/iEOAAJdi++jY4bmzsoGAKCl/rmuULe+kSdJeuwXw3VlBqdVA4A/a5P76AAAEEw2FDh119trJUk3Tkij5ABAEKHoAAA6pLJKt256NVe1Hp/O6J+oeyemmx0JANCKKDoAgA6n3uvTrW/kqeBQjdISovTUNafIZmXCGgAEE4oOAKDDyfl4i1bsLFNUmE0vTM6QIyLU7EgAgFZG0QEAdCjv5e3T3KW7JEmPXzVc/ZJiTE4EAGgLFB0AQIexocCpe99dL0m69ay+umBIN5MTAQDaCkUHANAhlFfV6aZXc+Wu9+msAYm647z+ZkcCALQhig4AIOg1DB9YrYJDNeoVH6knr2b4AAAEO4oOACDo5Xy8Rct3HB4+MCWT4QMA0AFQdAAAQe39vIImwwf6M3wAADoEig4AIGhtKHDqnnfXSZJuOasPwwcAoAOh6AAAgtL3hw+cOSBR2ecNMDsSAKAdUXQAAEHnh8MH/sbwAQDocCg6AICg86fDwwciw2x6fjLDBwCgI6LoAACCygdrCvTikeEDvxiuAV0ZPgAAHRFFBwAQNH44fGDiUIYPAEBHRdEBAASFI8MHaj0MHwAAUHQAAEHg+8MHesZH6m+TGD4AAB0dRQcAEPC+P3zghcmZckQyfAAAOjqKDgAgoDF8AABwLBQdAEDA2lj43fCBm89k+AAA4DsUHQBAQCqvqtOv5383fODOLIYPAAC+Q9EBAASceq9Pt73J8AEAwPFRdAAAAefPn2zRsu0MHwAAHB9FBwAQUD5YU6D/W9IwfOAxhg8AAI6DogMACBg/HD5wIcMHAADHEWJ2gBapqpJsNrNTAABMcLCqTrfPXS5LVa3O65egO8f3aPi9AADoWJr52W8xDMNo4ygnzeVyyeFwyCkp1uwwAAAAAEzjkuSQ5HQ6FRt7/HbAqWsAAAAAgk5gnbpWWCj9SGsDAASff64r1N1vN1yX89erRuiCoV1NTgQAMJXLJSUn/+RmgVV0oqIaFgBAh7Cx0Km7Fm5XbVi4fnNmH11wah+zIwEAzOb1NmszTl0DAPilg1V1uunVXNV6fDq9f6LuyhpgdiQAQACh6AAA/E6916fb3szTvoM1So2L1FNXj5DNajE7FgAggFB0AAB+59FPt2rp9lJFhNr0wpQMdYoMMzsSACDAUHQAAH7lgzUFeuHLnZKkx34xXAO7MoQGANByFB0AgN/YWOjUPe82TFj7zZl9dNGwbiYnAgAEKooOAMAvMHwAANCaKDoAANMxfAAA0NooOgAA0x0ZPhAZxvABAEDroOgAAEz1/eEDf7mS4QMAgNZB0QEAmIbhAwCAtkLRAQCYguEDAIC2RNEBALQ7hg8AANoaRQcA0O4YPgAAaGsUHQBAu2L4AACgPVB0AADthuEDAID2QtEBALSL7w8fOIPhAwCANkbRAQC0uaOHD5zC8AEAQJui6AAA2twPhw84IkPNjgQACHIUHQBAm2L4AADADBQdAECb2VDA8AEAgDkoOgCANnGgwq1fz1/F8AEAgCkoOgCAVldX79NvXstVobNWvROi9NQ1DB8AALQvig4AoFUZhqEHP9ygVXsOKiY8RP93XaYcEQwfAAC0L4oOAKBVvfrVHr35zV5ZLNJT15yiPonRZkcCAHRAFB0AQKtZvr1Uf/hokyTp3gsG6qwBXUxOBADoqCg6AIBWkV9WrZvfWC2vz9Blp3TXr0/vbXYkAEAHRtEBAJy0Sne9bpy/SoeqPRrew6Gcy4fKYmH4AADAPBQdAMBJ8fkMZS9Yo637K9Qlxq7nJ2cqPNRmdiwAQAd3QkVn9uzZSktLU3h4uDIyMrRkyZJm7bds2TKFhIRoxIgRJ/JtAQB+6Ml/f6vPNu1XmM2q5yZnqKsj3OxIAAC0vOgsWLBAM2fO1P3336+8vDxNmDBBEydOVH5+/o/u53Q6NWXKFJ1zzjknHBYA4F/+ta5IT/37W0nSHy8fqpGpnU1OBABAgxYXnSeeeELTpk3T9OnTlZ6erieffFIpKSmaM2fOj+5300036dprr9XYsWNPOCwAwH9sLHTqrrfXSpKmn5amKzN6mJwIAIDvtKjo1NXVKTc3V1lZWU3WZ2Vlafny5cfd76WXXtKOHTv04IMPNuv7uN1uuVyuJgsAwH+UVbr16/m5qvF4NaFfgu6dONDsSAAANNGiolNaWiqv16ukpKQm65OSklRcXHzMfb799lvde++9ev311xUSEtKs75OTkyOHw9G4pKSktCQmAKANueu9+s1rq1VwqEZpCVF65pqRCrEx2wYA4F9O6DfTD0eGGoZxzDGiXq9X1157rf7whz+of//+zf7vz5o1S06ns3HZu3fvicQEALQywzB03z826Jvd5YoJD9H/TcmUIzLU7FgAAByleYdYDktISJDNZjvq6E1JSclRR3kkqaKiQqtWrVJeXp5uvfVWSZLP55NhGAoJCdFnn32ms88++6j97Ha77HZ7S6IBANrBc4t36t3V+2SzWvTstSPVt0u02ZEAADimFh3RCQsLU0ZGhhYtWtRk/aJFizRu3Lijto+NjdX69eu1Zs2axmXGjBkaMGCA1qxZozFjxpxcegBAu/lkQ7Ee/XSLJOnBiwfp9P6JJicCAOD4WnRER5Kys7M1efJkZWZmauzYsXrhhReUn5+vGTNmSGo47aygoEDz58+X1WrVkCFDmuzfpUsXhYeHH7UeAOC/NhQ4dceCNTIMacrYnpoytpfZkQAA+FEtLjqTJk1SWVmZHn74YRUVFWnIkCFauHChevbsKUkqKir6yXvqAAACx35Xraa/sqpxwtrvfzbI7EgAAPwki2EYhtkhforL5ZLD4ZDT6VRsbKzZcQCgw6ip8+qq51dofYFTfbtE6x83j1NsOMMHAADmaW43YB4oAOCYfD5Dd769RusLnOocGaq512VScgAAAYOiAwA4pr9+vk0L1xcr1GbR85Mz1TM+yuxIAAA0G0UHAHCU9/L26en/bJck/fGyoRqdFmdyIgAAWoaiAwBoIndPue55Z70kacYZffSLzBSTEwEA0HIUHQBAoz1lVfr1/FzVeX3KGpSk350/wOxIAACcEIoOAECSdKi6Tte/tFJlVXUanByrv04aIavVYnYsAABOCEUHAKBaj1e/np+rnaVVSnaEa97UUYqyt/hWawAA+A2KDgB0cD6fobvfWadvdpcrxh6il64fraTYcLNjAQBwUig6ANDBPb5oqz5aW6gQq0VzfpWhAV1jzI4EAMBJo+gAQAf25jf5evaLHZKknMuH6rR+CSYnAgCgdVB0AKCDWrztgB54f4Mk6bfn9GOMNAAgqFB0AKAD2lTo0i2vr5bXZ+jyU7rrjnP7mR0JAIBWRdEBgA6myFmjG15eqUp3vcb2jtefrhgmi4Ux0gCA4ELRAYAOpKLWoxteXqViV636donWc7/KUFgIvwoAAMGH324A0EG467266dVcbS5yKSHarpemjpIjMtTsWAAAtAmKDgB0AD6foTvfWqvlO8oUFWbTS1NHKSUu0uxYAAC0GYoOAAQ5wzD0P//apH+uK1KozaLnJmdoaA+H2bEAAGhTFB0ACHLPf7lTLy3bLUl67BfDNaFformBAABoBxQdAAhi7+bu058+3iJJeuCidF0yorvJiQAAaB8UHQAIUl9sLdHv3l0nSfr16b01fUJvkxMBANB+KDoAEITW7D2km19ruCHoZad0170XDDQ7EgAA7YqiAwBBZueBSt3w8krVeLya0C9Bf75imKxWbggKAOhYKDoAEESKnbWaMu8blVfVaVgPBzcEBQB0WPz2A4AgUV5Vp1/N/Vr7DtaoV3yk5k0dpSh7iNmxAAAwBUUHAIJARa1H1837RttLKtXNEa7Xpo9RQrTd7FgAAJiGogMAAa7W49W0V1ZpfYFTcVFhenXaGPXoHGl2LAAATEXRAYAA5vH6dPPrq/XNrnLF2EM0/4bR6tsl2uxYAACYjqIDAAHK6zOU/dZa/WdLicJDrZo7dZSGdHeYHQsAAL9A0QGAAGQYhv7fBxv00dpChVgtmvOrDI1OizM7FgAAfoOiAwAB6M+fbNUbX+fLYpGevHqEzhrQxexIAAD4FYoOAASYp/79rZ5bvEOSlHPZUP1sWLLJiQAA8D8UHQAIILP/u11PLNomSXrgonRdPTrV5EQAAPgnig4ABIgXl+zUo59slSTdff4ATZ/Q2+REAAD4L4oOAASAV5bv1iP/2ixJmnluP91yVl+TEwEA4N8oOgDg517/eo8e/HCjJOmWs/ro9nP6mZwIAAD/R9EBAD/21sq9uv+9DZKkX5/eW3dlDZDFYjE5FQAA/o+iAwB+6r28fbrnH+skSdeP76VZEwdScgAAaCaKDgD4oQ/WFOjOt9bKMKRfnZqq3/9sECUHAIAWoOgAgJ95J3efZi5YI58hXT0qRQ//fAglBwCAFgoxOwAA4DsLVubr3n+sl2FI145J1SOXDJHVSskBAKClKDoA4Cde+2qPHni/YfDAdWN76qGfD+ZIDgAAJ4iiAwB+4OVlu/TQR5skSdNOS9MDF6VTcgAAOAkUHQAw2YtLdjbeDPSmM3rr3guYrgYAwMmi6ACAieb8d4f+/MkWSdJtZ/dV9nn9KTkAALQCig4AmMAwDD3+2TY988V2SdId5/bX7ef2MzkVAADBg6IDAO3M5zP00EcbNX/FHknS7y4YoJvP7GtyKgAAggtFBwDakcfr0+/eWaf38gpksUgPXzJEk0/taXYsAACCDkUHANpJrcerW99Yrc83lyjEatHjVw3XJSO6mx0LAICgRNEBgHZQ6a7X9FdW6qud5bKHWDXnVyN19sAks2MBABC0KDoA0MbKq+o09aVvtG6fU9H2EL14XaZO7R1vdiwAAIIaRQcA2tC+g9Wa+tJKbS+pVOfIUM2/YYyG9nCYHQsAgKBH0QGANrKp0KWpL32jkgq3ujnC9eq00erbJcbsWAAAdAgUHQBoA8u2l+qmV3NV6a7XgKQYvXzDKHVzRJgdCwCADoOiAwCt7P28At39zlp5vIbGpMXphSmZckSEmh0LAIAOhaIDAK3EMAy98OVO5Xy8RZJ00bBueuKq4bKH2ExOBgBAx0PRAYBW4PUZ+p9/btLLy3dLkqaflqb7LkyX1WoxNxgAAB0URQcATlKVu163/32NPt+8X5L0wEXpmj6ht8mpAADo2Cg6AHASipw1mvbyKm0qciksxKrHfzFcFw9PNjsWAAAdHkUHAE7Qun2HNP2VVSqpcCshOkwvTMnUyNTOZscCAACi6ADACVm4vkjZb61RrcenAUkxmjs1Uz06R5odCwAAHGY9kZ1mz56ttLQ0hYeHKyMjQ0uWLDnutv/4xz903nnnKTExUbGxsRo7dqw+/fTTEw4MAGYyDEPPfrFdN7++WrUen84ckKh3fjOWkgMAgJ9pcdFZsGCBZs6cqfvvv195eXmaMGGCJk6cqPz8/GNu/+WXX+q8887TwoULlZubq7POOksXX3yx8vLyTjo8ALSnWo9Xd761Vn/5dKsk6frxvfTilEzFhHOPHAAA/I3FMAyjJTuMGTNGI0eO1Jw5cxrXpaen69JLL1VOTk6z/huDBw/WpEmT9Pvf/75Z27tcLjkcDjmdTsXGxrYkLgC0isJDNbrp1VytL3DKZrXooZ8P1uRTe5odCwCADqe53aBF1+jU1dUpNzdX9957b5P1WVlZWr58ebP+Gz6fTxUVFYqLizvuNm63W263u/G5y+VqSUwAaFVf7SzTLa+vVllVnTpHhuqZa0dqfN8Es2MBAIAf0aJT10pLS+X1epWUlNRkfVJSkoqLi5v133j88cdVVVWlq6666rjb5OTkyOFwNC4pKSktiQkArcIwDL20bJd++eLXKquq06Busfrw1tMoOQAABIATGkZgsTS907dhGEetO5Y333xTDz30kBYsWKAuXbocd7tZs2bJ6XQ2Lnv37j2RmABwwmo9Xt359lr94aNN8voMXTIiWe/+ZpxS4hg6AABAIGjRqWsJCQmy2WxHHb0pKSk56ijPDy1YsEDTpk3T22+/rXPPPfdHt7Xb7bLb7S2JBgCtZm95tW55Y7XW7XPKapHuuzBd005La9Y/6AAAAP/QoiM6YWFhysjI0KJFi5qsX7RokcaNG3fc/d58801NnTpVb7zxhi666KITSwoA7eDzTfv1s6eXat0+pzpHhuq1aWM0fUJvSg4AAAGmxTcMzc7O1uTJk5WZmamxY8fqhRdeUH5+vmbMmCGp4bSzgoICzZ8/X1JDyZkyZYr+9re/6dRTT208GhQRESGHw9GKPwoAnDiP16fHPt2q57/cKUkantJJz1xzCqeqAQAQoFpcdCZNmqSysjI9/PDDKioq0pAhQ7Rw4UL17NkwZrWoqKjJPXWef/551dfX65ZbbtEtt9zSuP66667Tyy+/fPI/AQCcpCJnjW57I0+r9hyU1HB/nFkT0xUWckKXMQIAAD/Q4vvomIH76ABoK4u3HdAdC9aovKpOMfYQPXrlME0c2s3sWAAA4Dja5D46ABAsPF6fnvx8m2b/d4cMQxqcHKvZvxypnvFRZkcDAACtgKIDoMPZXVql2/+ep7X7nJKkX45J1f/72SCFh9pMTgYAAFoLRQdAh2EYht7O3aeHPtyo6jqvYsNDlHP5MF00jFPVAAAINhQdAB2Cs9qj+95br3+tL5IkjUmL018njVBypwiTkwEAgLZA0QEQ9L7aWabsBWtU6KxViNWi7Kz+uun0PrJZuTcOAADBiqIDIGjV1Hn12GdbNW/ZLhmGlJYQpScnjdDwlE5mRwMAAG2MogMgKOXuKdddb6/TrtIqSdKkzBT9/uJBirLzsQcAQEfAb3wAQaXW49UTi7bp/5bslGFISbF2/emKYTprQBezowEAgHZE0QEQNPLyD+qut9dqx4GGozhXZvTQ//vZIDkiQk1OBgAA2htFB0DAq66r118XbdPcpbvkM6TEGLv+dPlQnZOeZHY0AABgEooOgID2xZYSPfD+BhUcqpEkXToiWQ/9fLA6RYaZnAwAAJiJogMgIJVU1Orhjzbpn+sa7ovTvVOEHrl0iM4ayLU4AACAogMgwPh8hhas2quchZvlqq2X1SJNOy1Nd5zXX5FhfKQBAIAG/FUAIGBsLHTqwQ82atWeg5Kkod0dyrl8qIZ0d5icDAAA+BuKDgC/d7CqTo8v2qo3vs6Xz5Aiw2zKPq+/po7rpRCb1ex4AADAD1F0APgtr8/Qm9/k67HPtupQtUeS9LNh3XTfhelK7hRhcjoAAODPKDoA/NKq3eV68MON2ljokiQNSIrRQz8frLF94k1OBgAAAgFFB4BfyS+r1qOfbmmcphYbHqLs8/rrV6f25DQ1AADQbBQdAH7hUHWdnv7Pds1fsVseryGLRboqI0W/u2CA4qPtZscDAAABhqIDwFS1Hq/mr9itZ/6zXa7aeknShH4JmjUxXYOSY01OBwAAAhVFB4ApvD5D/1xXqL98ulX7DtZIkgZ2jdF9F6br9P6JJqcDAACBjqIDoF35fIY+3Visv36+Tdv2V0qSusaG686s/rp8ZA/ZrBaTEwIAgGBA0QHQLgzD0L83l+iJRdu0qahhklpseIhuOqOPbhifpogwm8kJAQBAMKHoAGhThmFoybelenzRNq3de0iSFG0P0Q2npWnaaWlyRISaGxAAAAQlig6ANmEYhv6zpUTPfrFdq/MPSZIiQm2aOr6Xfj2htzpHhZkbEAAABDWKDoBW5fUZ+tf6Is3+Yru2FFdIksJCrJp8ak/NOKOPEmMYFQ0AANoeRQdAq3DXe/WP1QV6fvEO7S6rltRwitovT03VtNPS1CUm3OSEAACgI6HoADgpzmqP3lyZr5eX7Vaxq1aS1DkyVDeMT9OUsb3kiOQaHAAA0P4oOgBOyM4DlXpp2W69k7tPNR6vJCkp1q4bJ/TWtWNSFRnGxwsAADAPf4kAaDbDMLR8R5nmLd2lf28paVw/sGuMbhifpktOSZY9hDHRAADAfBQdAD+pyl2vD9YUav6K3Y0DBiTpnIFdNO20NI3tEy+LhRt9AgAA/0HRAXBcm4tcev3rPXo/r1CV7npJDSOif5HZQ1PH9VLvxGiTEwIAABwbRQdAE7Uer/61rkivf72n8f43kpSWEKVrR6fqqswUBgwAAAC/R9EBIMMwtL7AqXdz9+n9NYVy1ngkSSFWi84f3FW/HJPK6WkAACCgUHSADqzEVav38gr07up92ra/snF9904RunZMqn6R2YP73wAAgIBE0QE6mFqPV4s27de7q/fpy20H5DMa1ttDrMoa3FVXjOyuCf0SZbNy9AYAAAQuig7QAdTV+7R0+wH9c22RPtu0v3GwgCRl9OysK0b20EXDuskRwbU3AAAgOFB0gCBV7/Vpxc4y/XNtkT7ZWNx43Y0kJTvCdfnIHrp8ZHcmpwEAgKBE0QGCiMfr09c7y/XJxiJ9vL5YZVV1ja8lRNt10dCu+tnwZGWkdpaVU9MAAEAQo+gAAa6i1qPF2w7os4379cXWElXUfndaWlxUmC4Y0lU/G9ZNY9Liue4GAAB0GBQdIADtd9Vq0ab9WrRpv1bsKFOd19f4WkJ0mM5NT9KFQ7tpbJ94hdqsJiYFAAAwB0UHCAB19T7l7jmoxdsOaPG2A9pc5Gryeu+EKJ03OElZg5I0IqUzR24AAECHR9EB/NTe8urGYrN8e6mq6rxNXh+R0klZg5OUNair+nZhoAAAAMD3UXQAP1FW6dZXO8u1Ymeplm8v087Sqiavx0eF6fT+iTqjf6JO65eghGi7SUkBAAD8H0UHMMnBqjp9vatMK3aUacXOMm3bX9nkdZvVopGpnXRG/0Sd0b+LBifHMikNAACgmSg6QDspPFSj3D0HlbvnoL7eVa4txS4ZRtNtBnaN0am943Vq73iN7RPPDTwBAABOEEUHaAMer0+bCl0NxSb/oFbvOagiZ+1R2/XrEq2xfeI1tne8RqfFKZ7T0QAAAFoFRQc4SYZhKL+8Wuv2ObWhwKm8vYe0bt8h1Xp8TbazWS0anByrkamdldmrs8akxSsxhmIDAADQFig6QAsYhqG95TVaX+DUuoJD2lDg1Pp9Trm+d5POIzpFhiojtbNG9uysjJ6dNayHQ5Fh/C8HAADQHvirCziOWo9X3+6v1JZil7YUV2hrcYXWFzjlrPEctW1YiFXp3WI1tHushvXopIyendU7IUoWC8MDAAAAzEDRQYfn8xnae7BaW4ortKWoQlv3NxSb3aVV8hlHbx9ms2pgtxgN7e5oWHo41D8pRqE2a/uHBwAAwDFRdNBh1NR5tbO0UjsOVGnngYbHHSWV2lVapRqP95j7xEWFaUBSjAZ0jdHArjEa0r2h1ISFUGoAAAD8GUUHQcXj9anwUI3yy6u1u7SqocwcqNTOA1UqOFRz3P3CQqzq1yVaA7vGamDXw8WmW4wSo+2cfgYAABCAKDoIOM5qj/LLq7WnvEr55dXaW17d8LysWoWHao55utkRnSJD1TcxWr0To9QnMVp9Dn+dGhepEE49AwAACBoUHfgVj9en/a5aFTlrVXioRoWHalXk/O5xb3n1MSecfZ89xKrUuEj1jI9U78Ro9TlcanonRisuKqydfhIAAACYiaKDdmEYhqrqvCpx1aqkwq0DFW6VVLhVdKimodQ4a1R4qEYHKtw/ekTmiMQYu1LjIpUaF6mUuEj1jItUanzD88Rou6xWTjcDAADoyCg6OCnueq8OVnlUVvVdeTnQWGRqG9eVuNzHveD/h0JtFnV1hKubI0LdO0WomyNc3TpFKNkRru6dI5QaF8n9aAAAAPCj+GsRjXw+Q65aj8qr6hqXg9V1Kquq08Gq7x7Lqz0qr3LrYJVHle4fP43sh6LCbOoSG67EaLsSY+3qFvtdienWKULJncKVEMURGQAAAJwcik6QqfV45arxyFnjkavWI1dNfePXzurDjzVN1x95rcJdL6MZp439kM1qUefIMCXG2NUlxv6Dx3B1ibU3FJsYu6LsvOUAAADQ9vir02Qer081Hq+q3PWqcter0u09/Fh//HV13607sv7Iax7vCTSVH4ixh6hzVJjifrB0jgxTfFTY4ddCFRdlV1xkmGLCQzgCAwAAAL9yQkVn9uzZ+stf/qKioiINHjxYTz75pCZMmHDc7RcvXqzs7Gxt3LhRycnJ+t3vfqcZM2accGizGIahr3eVq8bjldvjVY3Hq5q6hqJS6/Gqpu7wOo9Xtd/7uqbu8Oser2o9viav1zfnyvsWslik2PBQOSJCFRsR0vDY+PzwY3iIYg8//+G29hBbq2cCAAAA2lOLi86CBQs0c+ZMzZ49W+PHj9fzzz+viRMnatOmTUpNTT1q+127dunCCy/UjTfeqNdee03Lli3TzTffrMTERF1xxRWt8kO0F4vFol+9+HWblJMQq0VR9hBF20MUZbd993VYyOGvG9Z9t82x18WEhyg6jCMsAAAA6NgshtGyqzLGjBmjkSNHas6cOY3r0tPTdemllyonJ+eo7e+55x59+OGH2rx5c+O6GTNmaO3atVqxYkWzvqfL5ZLD4ZDT6VRsbGxL4ra6nz+zVD7DUESoTeGHl4gjS9j3nodZG7eJCPtuG/v3tv1unVX2EKssFsoJAAAA8GOa2w1adESnrq5Oubm5uvfee5usz8rK0vLly4+5z4oVK5SVldVk3fnnn6+5c+fK4/EoNDT0qH3cbrfcbneTH8ZffHjraWZHAAAAAPATrC3ZuLS0VF6vV0lJSU3WJyUlqbi4+Jj7FBcXH3P7+vp6lZaWHnOfnJwcORyOxiUlJaUlMQEAAAB0cC0qOkf88BQrwzB+9LSrY21/rPVHzJo1S06ns3HZu3fvicQEAAAA0EG16NS1hIQE2Wy2o47elJSUHHXU5oiuXbsec/uQkBDFx8cfcx+73S673d6SaAAAAADQqEVHdMLCwpSRkaFFixY1Wb9o0SKNGzfumPuMHTv2qO0/++wzZWZmHvP6HAAAAAA4WS0+dS07O1svvvii5s2bp82bN+uOO+5Qfn5+431xZs2apSlTpjRuP2PGDO3Zs0fZ2dnavHmz5s2bp7lz5+quu+5qvZ8CAAAAAL6nxffRmTRpksrKyvTwww+rqKhIQ4YM0cKFC9WzZ09JUlFRkfLz8xu3T0tL08KFC3XHHXfo2WefVXJysp566qmAu4cOAAAAgMDR4vvomMGf7qMDAAAAwDzN7QYnNHUNAAAAAPwZRQcAAABA0KHoAAAAAAg6FB0AAAAAQYeiAwAAACDoUHQAAAAABB2KDgAAAICg0+IbhprhyK1+XC6XyUkAAAAAmOlIJ/ip24EGRNGpqKiQJKWkpJicBAAAAIA/qKiokMPhOO7rFuOnqpAf8Pl8KiwsVExMjCwWi9lxcBwul0spKSnau3fvj96lFpB4v6DleM+gpXjPoKV4zwQGwzBUUVGh5ORkWa3HvxInII7oWK1W9ejRw+wYaKbY2Fg+HNBsvF/QUrxn0FK8Z9BSvGf8348dyTmCYQQAAAAAgg5FBwAAAEDQoeig1djtdj344IOy2+1mR0EA4P2CluI9g5biPYOW4j0TXAJiGAEAAAAAtARHdAAAAAAEHYoOAAAAgKBD0QEAAAAQdCg6AAAAAIIORQcAAABA0KHooE253W6NGDFCFotFa9asMTsO/NTu3bs1bdo0paWlKSIiQn369NGDDz6ouro6s6PBj8yePVtpaWkKDw9XRkaGlixZYnYk+KmcnByNGjVKMTEx6tKliy699FJt3brV7FgIEDk5ObJYLJo5c6bZUXCSKDpoU7/73e+UnJxsdgz4uS1btsjn8+n555/Xxo0b9de//lXPPfec7rvvPrOjwU8sWLBAM2fO1P3336+8vDxNmDBBEydOVH5+vtnR4IcWL16sW265RV999ZUWLVqk+vp6ZWVlqaqqyuxo8HMrV67UCy+8oGHDhpkdBa2A++igzXz88cfKzs7Wu+++q8GDBysvL08jRowwOxYCxF/+8hfNmTNHO3fuNDsK/MCYMWM0cuRIzZkzp3Fdenq6Lr30UuXk5JiYDIHgwIED6tKlixYvXqzTTz/d7DjwU5WVlRo5cqRmz56tRx55RCNGjNCTTz5pdiycBI7ooE3s379fN954o1599VVFRkaaHQcByOl0Ki4uzuwY8AN1dXXKzc1VVlZWk/VZWVlavny5SakQSJxOpyTxmYIfdcstt+iiiy7Sueeea3YUtJIQswMg+BiGoalTp2rGjBnKzMzU7t27zY6EALNjxw49/fTTevzxx82OAj9QWloqr9erpKSkJuuTkpJUXFxsUioECsMwlJ2drdNOO01DhgwxOw781N///netXr1aK1euNDsKWhFHdNBsDz30kCwWy48uq1at0tNPPy2Xy6VZs2aZHRkma+575vsKCwt1wQUX6Be/+IWmT59uUnL4I4vF0uS5YRhHrQN+6NZbb9W6dev05ptvmh0Ffmrv3r26/fbb9dprryk8PNzsOGhFXKODZistLVVpaemPbtOrVy9dffXV+uijj5r8AeL1emWz2fTLX/5Sr7zySltHhZ9o7nvmyC+WwsJCnXXWWRozZoxefvllWa38WwwaTl2LjIzU22+/rcsuu6xx/e233641a9Zo8eLFJqaDP7vtttv0/vvv68svv1RaWprZceCn3n//fV122WWy2WyN67xerywWi6xWq9xud5PXEDgoOmh1+fn5crlcjc8LCwt1/vnn65133tGYMWPUo0cPE9PBXxUUFOiss85SRkaGXnvtNX6poIkxY8YoIyNDs2fPblw3aNAgXXLJJQwjwFEMw9Btt92m9957T//973/Vr18/syPBj1VUVGjPnj1N1l1//fUaOHCg7rnnHk55DGBco4NWl5qa2uR5dHS0JKlPnz6UHBxTYWGhzjzzTKWmpuqxxx7TgQMHGl/r2rWricngL7KzszV58mRlZmZq7NixeuGFF5Sfn68ZM2aYHQ1+6JZbbtEbb7yhDz74QDExMY3XcjkcDkVERJicDv4mJibmqDITFRWl+Ph4Sk6Ao+gAMN1nn32m7du3a/v27UeVYQ46Q5ImTZqksrIyPfzwwyoqKtKQIUO0cOFC9ezZ0+xo8ENHxpCfeeaZTda/9NJLmjp1avsHAmAKTl0DAAAAEHS40hcAAABA0KHoAAAAAAg6FB0AAAAAQYeiAwAAACDoUHQAAAAABB2KDgAAAICgQ9EBAAAAEHQoOgAAAACCDkUHAAAAQNCh6AAAAAAIOhQdAAAAAEHn/wOLGUZ64kijkAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating vectors X and Y\n",
    "x = np.linspace(-5, 5, 10000)\n",
    "y = sigmoid(x)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "# Create the plot\n",
    "plt.plot(x, y)\n",
    "# Show the plot\n",
    "plt.axhline(.5, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression is going to use this sigmoid functions to generate a prediction between 0 and 1. Can can plug the linear regression equation into the sigmoid function, then our new hypothesis becomes:\n",
    "\n",
    "$ y = \\frac{1}{(1+e^(m*x+b))} $\n",
    "\n",
    "Where y = probability. \n",
    "\n",
    "Note - the mathmatical derivations aren't really super-duper critical. If it is confusing, just ingore it. There's a full derivation and example here: https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Real Example - SciKitLearn</h2>\n",
    "\n",
    "For our first try we can use the diabetes example we've used a bit before, though we've always sidesteped the true target. The outcome value is whether or not someone is diabetic, and all the other variables that are risk factors that we can use to predict if someone will become diabetic. Our aim is to predict, yes or no, will someone develop diabetes based on those risk factors. \n",
    "\n",
    "First - one variable. We'll use BMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 1) (768, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df[\"Outcome\"]).reshape(-1,1)\n",
    "x = np.array(df[\"BMI\"]).reshape(-1,1)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>ravel()</b> - sometimes you may get a message that says something like \"we want the y data in the shape (samples,)\". This is obviously a contradiction to what we said to always shape the y as (samples, 1). The easiest way to deal with this is to use the .ravel() function as shown below. The cause is the expectation of data format for whatever you're using, which can vary. If we always make the y array (samples, 1) and then use ravel when needed, that allows us to be consistent and not worry about it much. I'd suggest keeping with this for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6496062992125984\n"
     ]
    }
   ],
   "source": [
    "md1 = LogisticRegression().fit(X_train,y_train.ravel())\n",
    "md1Pred = md1.predict(X_test)\n",
    "\n",
    "score = md1.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66% accuracy. Not bad. We can try with more Xs though...\n",
    "\n",
    "#### Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all the X values. \n",
    "# I can use the y from above still \n",
    "df2 = df.drop(columns={\"Outcome\"})\n",
    "x2 = np.array(df2)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7677165354330708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akeem/anaconda3/envs/ml_2/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#I'm reusing some varaible names to make my life easier with copy/paste. \n",
    "#Make sure you run things in order if you do this. \n",
    "X_train, X_test, y_train, y_test = train_test_split(x2, y, test_size=0.33)\n",
    "\n",
    "md2 = LogisticRegression().fit(X_train,y_train.ravel())\n",
    "md2Pred = md2.predict(X_test)\n",
    "score = md2.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this the first time, I didn't get an answer, I instead got something along the lines of \"failed to converge\". This means that the gradient descent process didn't finish, and the algorithm didn't settle on an answer. We will explore this more in the machine learning stuff, for now we can just tell it to set a higher cap on how long it can run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7677165354330708\n"
     ]
    }
   ],
   "source": [
    "md2 = LogisticRegression(max_iter=1000).fit(X_train,y_train.ravel())\n",
    "md2Pred = md2.predict(X_test)\n",
    "score = md2.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results \n",
    "\n",
    "We can demonstrate some results... We'll look into result details more later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix \n",
    "preds = md2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       165\n",
      "           1       0.70      0.58      0.64        89\n",
      "\n",
      "    accuracy                           0.77       254\n",
      "   macro avg       0.75      0.73      0.73       254\n",
      "weighted avg       0.76      0.77      0.76       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143  22]\n",
      " [ 37  52]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.77165354330708\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, preds)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Work Through Titanic</h1>\n",
    "\n",
    "Predict who lives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data\n",
    "dfe = pd.read_csv(\"data/train.csv\")\n",
    "dfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe[\"no_cabin\"] = dfe[\"Cabin\"].isnull()\n",
    "dfe[\"family\"] = dfe[\"SibSp\"] + dfe[\"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dropped age due to missing values. Think about if there's anything else we may want to do to it instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>no_cabin</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex     Fare Embarked  no_cabin  family\n",
       "0         0       3    male   7.2500        S      True       1\n",
       "1         1       1  female  71.2833        C     False       1\n",
       "2         1       3  female   7.9250        S      True       0\n",
       "3         1       1  female  53.1000        S     False       1\n",
       "4         0       3    male   8.0500        S      True       0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfe2 = dfe.drop(columns={\"Name\", \"Ticket\", \"Cabin\", \"SibSp\", \"Parch\", \"PassengerId\", \"Age\"})\n",
    "dfe2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 7), (891, 1))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Redo the dummy variables. \n",
    "dfe2_dumb = pd.get_dummies(dfe2, drop_first=True)\n",
    "\n",
    "ye = np.array(dfe2_dumb[\"Survived\"]).reshape(-1,1)\n",
    "xe = dfe2_dumb.drop(columns={\"Survived\"})\n",
    "xe.shape, ye.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8161434977578476"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainT, X_testT, y_trainT, y_testT = train_test_split(xe, ye)\n",
    "\n",
    "titan = LogisticRegression(max_iter=1000).fit(X_trainT,y_trainT.ravel())\n",
    "titan_preds = titan.predict(X_testT)\n",
    "scoreT = titan.score(X_testT, y_testT)\n",
    "scoreT"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
